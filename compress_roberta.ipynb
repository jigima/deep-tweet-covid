{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1454eb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.venv/lib/python3.10/site-packages (4.53.3)\n",
      "Requirement already satisfied: evaluate in ./.venv/lib/python3.10/site-packages (0.4.5)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.10/site-packages (1.7.1)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.10/site-packages (1.15.3)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.venv/lib/python3.10/site-packages (from transformers) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.10/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: datasets>=2.0.0 in ./.venv/lib/python3.10/site-packages (from evaluate) (4.0.0)\n",
      "Requirement already satisfied: dill in ./.venv/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.10/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in ./.venv/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (21.0.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.14)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.10)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (2025.7.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers evaluate scikit-learn scipy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bbe4baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import pandas as pd \n",
    "from torch import nn\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "PRETRAIN_MODEL = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "BEST_TRIAL = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eff69f8",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500f4b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/split_train_data.csv\").dropna(subset=[\"Label\"]).reset_index(drop=True)\n",
    "eval_df = pd.read_csv(\"data/split_eval_data.csv\").dropna(subset=[\"Label\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6219d6",
   "metadata": {},
   "source": [
    "#### Load Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd758d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "CKPT_PT = f\"models/training/roberta_best_model_trial_{BEST_TRIAL}.pt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAIN_MODEL, use_fast=True)\n",
    "best_model = AutoModelForSequenceClassification.from_pretrained(PRETRAIN_MODEL, num_labels=5, ignore_mismatched_sizes=True)\n",
    "\n",
    "state = torch.load(CKPT_PT, map_location=\"cpu\")\n",
    "best_model.load_state_dict(state) \n",
    "best_model.eval()\n",
    "baseline_num_params = sum(p.numel() for p in best_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e39fd",
   "metadata": {},
   "source": [
    "#### 1. QUANTIZATION DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a02b2990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model params: 124649477\n",
      "Quantized model params: 39037440\n"
     ]
    }
   ],
   "source": [
    "from torch.ao.quantization import quantize_dynamic\n",
    "\n",
    "# Apply dynamic quantization on Linear layers\n",
    "quantized_model = quantize_dynamic(best_model, {nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "quantized_num_params = sum(p.numel() for p in quantized_model.parameters())\n",
    "print(\"Original model params:\", baseline_num_params)\n",
    "print(\"Quantized model params:\", quantized_num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9798d85",
   "metadata": {},
   "source": [
    "#### 2. Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e37049fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning applied: 40% of weights zeroed out in Linear layers.\n",
      "Remaining non-zero parameters: 99168312\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils import prune\n",
    "\n",
    "# Only prune FFN dense layers, not attention or classifier\n",
    "def collect_ffn_linears(model):\n",
    "    pairs = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if \".intermediate.dense\" in name or \".output.dense\" in name:\n",
    "                pairs.append((module, \"weight\"))\n",
    "    return pairs\n",
    "\n",
    "model_to_prune = copy.deepcopy(best_model)\n",
    "parameters_to_prune = collect_ffn_linears(model_to_prune)\n",
    "\n",
    "# Global L1 pruning across FFN weights (40%)\n",
    "prune.global_unstructured(parameters_to_prune, prune.L1Unstructured, amount=0.4)\n",
    "\n",
    "# Remove masks (make pruning permanent)\n",
    "for m, _ in parameters_to_prune:\n",
    "    prune.remove(m, \"weight\")\n",
    "\n",
    "\n",
    "print(\"Pruning applied: 40% of weights zeroed out in Linear layers.\")\n",
    "pruned_num_params = sum((p != 0).sum().item() for p in model_to_prune.parameters())\n",
    "print(\"Remaining non-zero parameters:\", pruned_num_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405450dc",
   "metadata": {},
   "source": [
    "#### 3 Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22da2cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilbert/distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4988335423334ad891c84ed8d81deb2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28808 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09fdd9270140428a88a6a2993715461b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Teacher = your best trained model\n",
    "teacher = best_model.to(device)\n",
    "teacher.eval()\n",
    "for p in teacher.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "# Student = smaller pre-trained model\n",
    "student_name = \"distilbert/distilroberta-base\"\n",
    "student = AutoModelForSequenceClassification.from_pretrained(student_name, num_labels=5).to(device)\n",
    "\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, temperature=2.0, alpha=0.5, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs_student = model(**inputs)\n",
    "        with torch.no_grad():\n",
    "            #outputs_teacher = self.teacher(**inputs)\n",
    "            #outputs_teacher = self.teacher(**inputs)\n",
    "            outputs_teacher = self.teacher(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs.get(\"attention_mask\", None)\n",
    "            )\n",
    "\n",
    "        labels = labels.to(outputs_student.logits.device).long().view(-1)\n",
    "        # labels = inputs[\"labels\"].to(outputs_student.logits.device)\n",
    "        # labels = labels.long().view(-1)\n",
    "\n",
    "        loss_ce = F.cross_entropy(outputs_student.logits, labels)\n",
    "        loss_kl = F.kl_div(\n",
    "            F.log_softmax(outputs_student.logits / self.temperature, dim=-1),\n",
    "            F.softmax(outputs_teacher.logits / self.temperature, dim=-1),\n",
    "            reduction=\"batchmean\",\n",
    "        ) * (self.temperature ** 2)\n",
    "\n",
    "        loss = self.alpha * loss_ce + (1 - self.alpha) * loss_kl\n",
    "        return (loss, outputs_student) if return_outputs else loss\n",
    "\n",
    "# Wrap your datasets in torch Dataset -> DataLoader is handled by Trainer\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "train_hf = HFDataset.from_pandas(train_df.rename(columns={\"Label\": \"labels\"}))\n",
    "eval_hf = HFDataset.from_pandas(eval_df.rename(columns={\"Label\": \"labels\"}))\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    texts = [str(x) for x in batch[\"OriginalTweet\"]] \n",
    "    tokenized = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128)\n",
    "    tokenized[\"labels\"] = [int(x) for x in batch[\"labels\"]]\n",
    "    return tokenized\n",
    "\n",
    "#train_tok.reset_format()\n",
    "#eval_tok.reset_format()\n",
    "train_tok = train_hf.map(tokenize_fn, batched=True)\n",
    "eval_tok = eval_hf.map(tokenize_fn, batched=True)\n",
    "train_tok.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "eval_tok.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b13b918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myoyulia\u001b[0m (\u001b[33myoyulia-tel-aviv-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/idok/git/deep-tweet-covid/wandb/run-20250821_215624-kjy16h36</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yoyulia-tel-aviv-university/huggingface/runs/kjy16h36' target=\"_blank\">./distill_out</a></strong> to <a href='https://wandb.ai/yoyulia-tel-aviv-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yoyulia-tel-aviv-university/huggingface' target=\"_blank\">https://wandb.ai/yoyulia-tel-aviv-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yoyulia-tel-aviv-university/huggingface/runs/kjy16h36' target=\"_blank\">https://wandb.ai/yoyulia-tel-aviv-university/huggingface/runs/kjy16h36</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5403' max='5403' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5403/5403 14:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.687300</td>\n",
       "      <td>0.721515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.442800</td>\n",
       "      <td>0.679479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.331700</td>\n",
       "      <td>0.604971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5403, training_loss=0.6221571788839911, metrics={'train_runtime': 879.6874, 'train_samples_per_second': 98.244, 'train_steps_per_second': 6.142, 'total_flos': 2862243739330560.0, 'train_loss': 0.6221571788839911, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./distill_out\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "\n",
    "trainer = DistillationTrainer(\n",
    "    model=student,\n",
    "    teacher_model=teacher,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=eval_tok,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "143ce03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distillation complete. Student model trained.\n",
      "Student model size: 124649477\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDistillation complete. Student model trained.\")\n",
    "distilled_num_params = sum(p.numel() for p in best_model.parameters())\n",
    "print(\"Student model size:\", distilled_num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e55d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "model_name = PRETRAIN_MODEL.split(\"/\")[1]\n",
    "output_model_path = f\"models/{model_name}-full\"\n",
    "\n",
    "os.makedirs(output_model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c708e481",
   "metadata": {},
   "source": [
    "### Save compress models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67de82ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/2-twitter-roberta-base-sentiment-full/distilled/tokenizer_config.json',\n",
       " 'models/2-twitter-roberta-base-sentiment-full/distilled/special_tokens_map.json',\n",
       " 'models/2-twitter-roberta-base-sentiment-full/distilled/vocab.json',\n",
       " 'models/2-twitter-roberta-base-sentiment-full/distilled/merges.txt',\n",
       " 'models/2-twitter-roberta-base-sentiment-full/distilled/added_tokens.json',\n",
       " 'models/2-twitter-roberta-base-sentiment-full/distilled/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline\n",
    "best_model.save_pretrained(f\"{output_model_path}/baseline\")\n",
    "tokenizer.save_pretrained(f\"{output_model_path}/baseline\")\n",
    "\n",
    "# Quantized\n",
    "torch.save(\n",
    "    quantized_model.state_dict(),\n",
    "    f\"{output_model_path}/quantized.pt\"\n",
    ")\n",
    "tokenizer.save_pretrained(f\"{output_model_path}/quantized\")  # same tokenizer\n",
    "\n",
    "# Pruned\n",
    "model_to_prune.save_pretrained(f\"{output_model_path}/pruned\")\n",
    "tokenizer.save_pretrained(f\"{output_model_path}/pruned\")\n",
    "\n",
    "# Distilled\n",
    "student.save_pretrained(f\"{output_model_path}/distilled\")\n",
    "tokenizer.save_pretrained(f\"{output_model_path}/distilled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc0d32d",
   "metadata": {},
   "source": [
    "### Load compress models (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c487e15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# # Load Baseline\n",
    "# student = AutoModelForSequenceClassification.from_pretrained(f\"{output_model_path}/baseline\")\n",
    "# distilled_tokenizer = AutoTokenizer.from_pretrained(f\"{output_model_path}/baseline\")\n",
    "\n",
    "# # Load Quantized (CPU only!)\n",
    "# quantized_tokenizer = AutoTokenizer.from_pretrained(f\"{output_model_path}/quantized\")\n",
    "# quantized_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     PRETRAIN_MODEL, num_labels=5\n",
    "# )\n",
    "# quantized_model.load_state_dict(torch.load(f\"{output_model_path}/quantized.pt\"))\n",
    "# quantized_model = quantize_dynamic(quantized_model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "# # Load Pruned\n",
    "# model_to_prune = AutoModelForSequenceClassification.from_pretrained(f\"{output_model_path}/pruned\")\n",
    "# pruned_tokenizer = AutoTokenizer.from_pretrained(f\"{output_model_path}/pruned\")\n",
    "\n",
    "# # Load Distilled\n",
    "# student = AutoModelForSequenceClassification.from_pretrained(f\"{output_model_path}/distilled\")\n",
    "# distilled_tokenizer = AutoTokenizer.from_pretrained(f\"{output_model_path}/distilled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa7b9af",
   "metadata": {},
   "source": [
    "### Compare the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f93c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▅▁</td></tr><tr><td>eval/runtime</td><td>▁▆█</td></tr><tr><td>eval/samples_per_second</td><td>█▃▁</td></tr><tr><td>eval/steps_per_second</td><td>█▃▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▄▇▆▇▆▅▄█▃▄▄▃▃▅▁▄▂▃▁▂▄▁▅▂▁▃▂▃▅▄▄▃▄▂▂▂▂▂▂▃</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▅▄▅▄▃▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.60497</td></tr><tr><td>eval/runtime</td><td>53.2275</td></tr><tr><td>eval/samples_per_second</td><td>231.985</td></tr><tr><td>eval/steps_per_second</td><td>14.504</td></tr><tr><td>total_flos</td><td>2862243739330560.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>5403</td></tr><tr><td>train/grad_norm</td><td>11.6251</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3317</td></tr><tr><td>train_loss</td><td>0.62216</td></tr><tr><td>train_runtime</td><td>879.6874</td></tr><tr><td>train_samples_per_second</td><td>98.244</td></tr><tr><td>train_steps_per_second</td><td>6.142</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">./distill_out</strong> at: <a href='https://wandb.ai/yoyulia-tel-aviv-university/huggingface/runs/kjy16h36' target=\"_blank\">https://wandb.ai/yoyulia-tel-aviv-university/huggingface/runs/kjy16h36</a><br> View project at: <a href='https://wandb.ai/yoyulia-tel-aviv-university/huggingface' target=\"_blank\">https://wandb.ai/yoyulia-tel-aviv-university/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250821_215624-kjy16h36/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/idok/git/deep-tweet-covid/wandb/run-20250821_221440-dlx63i09</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yoyulia-tel-aviv-university/model_compression-twitter-roberta-base-sentiment/runs/dlx63i09' target=\"_blank\">Baseline-Quantized-Pruned-Distilled-twitter-roberta-base-sentiment</a></strong> to <a href='https://wandb.ai/yoyulia-tel-aviv-university/model_compression-twitter-roberta-base-sentiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yoyulia-tel-aviv-university/model_compression-twitter-roberta-base-sentiment' target=\"_blank\">https://wandb.ai/yoyulia-tel-aviv-university/model_compression-twitter-roberta-base-sentiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yoyulia-tel-aviv-university/model_compression-twitter-roberta-base-sentiment/runs/dlx63i09' target=\"_blank\">https://wandb.ai/yoyulia-tel-aviv-university/model_compression-twitter-roberta-base-sentiment/runs/dlx63i09</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='772' max='772' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [772/772 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='772' max='772' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [772/772 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='772' max='772' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [772/772 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idok/git/deep-tweet-covid/.venv/lib/python3.10/site-packages/transformers/training_args.py:1604: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='772' max='772' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [772/772 07:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "\"['params'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Put results in DataFrame\u001b[39;00m\n\u001b[1;32m     83\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n\u001b[0;32m---> 84\u001b[0m results_df \u001b[38;5;241m=\u001b[39m \u001b[43mresults_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_accuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_precision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_recall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_f1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_auc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/git/deep-tweet-covid/.venv/lib/python3.10/site-packages/pandas/core/frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4112\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4113\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4115\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/git/deep-tweet-covid/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:6212\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6210\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6212\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6214\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6216\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/git/deep-tweet-covid/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:6264\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6263\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6264\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['params'] not in index\""
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "model_name = PRETRAIN_MODEL.split(\"/\")[1]\n",
    "wandb.init(\n",
    "    project=f\"model_compression-{model_name}\",\n",
    "    name=f\"Baseline-Quantized-Pruned-Distilled-{model_name}\",\n",
    "    mode=\"online\"  # use \"offline\" if no internet\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import Trainer\n",
    "\n",
    "# Define compute_metrics\n",
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "metric_precision = evaluate.load(\"precision\")\n",
    "metric_recall = evaluate.load(\"recall\")\n",
    "metric_f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_ce_loss(outputs, labels, num_items_in_batch=None):\n",
    "    labels = labels.long().view(-1)\n",
    "    logits = outputs.logits\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    accuracy = metric_acc.compute(predictions=predictions, references=labels)\n",
    "    precision = metric_precision.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    recall = metric_recall.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    f1 = metric_f1.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "    probs = softmax(logits, axis=1)\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs, multi_class=\"ovr\", average=\"macro\")\n",
    "    except ValueError:\n",
    "        auc = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"precision\": precision[\"precision\"],\n",
    "        \"recall\": recall[\"recall\"],\n",
    "        \"f1\": f1[\"f1\"],\n",
    "        \"auc\": auc,\n",
    "    }\n",
    "\n",
    "# Create lightweight Trainer wrapper for evaluation\n",
    "def evaluate_model(model, name):\n",
    "    use_cuda = torch.cuda.is_available() and name != \"Quantized\"\n",
    "    model.to(\"cuda\" if use_cuda else \"cpu\")\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"./tmp\",\n",
    "        per_device_eval_batch_size=16,\n",
    "        no_cuda=not use_cuda,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(model=model, args=args, eval_dataset=eval_tok, compute_metrics=compute_metrics, compute_loss_func=compute_ce_loss)\n",
    "    results = trainer.evaluate()\n",
    "    \n",
    "    dense_params = sum(p.numel() for p in model.parameters())\n",
    "    nonzero_params = sum((p != 0).sum().item() for p in model.parameters())\n",
    "    results.update({\n",
    "        \"params_dense\": dense_params,\n",
    "        \"params_nonzero\": nonzero_params,\n",
    "        \"model\": name,\n",
    "    })\n",
    "    return results\n",
    "\n",
    "# Evaluate all models\n",
    "results = []\n",
    "results.append(evaluate_model(model_to_prune, \"Pruned\"))\n",
    "results.append(evaluate_model(student, \"Distilled\"))\n",
    "results.append(evaluate_model(best_model, \"Baseline\"))\n",
    "results.append(evaluate_model(quantized_model, \"Quantized\"))\n",
    "\n",
    "\n",
    "# Put results in DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df[[\"model\", \"params_dense\", \"params_nonzero\",\n",
    "                         \"eval_loss\", \"eval_accuracy\", \"eval_precision\",\n",
    "                         \"eval_recall\", \"eval_f1\", \"eval_auc\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f5ea80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params_dense</th>\n",
       "      <th>params_nonzero</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pruned</td>\n",
       "      <td>124649477</td>\n",
       "      <td>99168312</td>\n",
       "      <td>1.292657</td>\n",
       "      <td>0.580661</td>\n",
       "      <td>0.714066</td>\n",
       "      <td>0.527425</td>\n",
       "      <td>0.513991</td>\n",
       "      <td>0.896321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Distilled</td>\n",
       "      <td>82122245</td>\n",
       "      <td>82121477</td>\n",
       "      <td>0.576013</td>\n",
       "      <td>0.799401</td>\n",
       "      <td>0.801039</td>\n",
       "      <td>0.821834</td>\n",
       "      <td>0.805277</td>\n",
       "      <td>0.964596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>124649477</td>\n",
       "      <td>124648709</td>\n",
       "      <td>0.788653</td>\n",
       "      <td>0.722708</td>\n",
       "      <td>0.724727</td>\n",
       "      <td>0.748257</td>\n",
       "      <td>0.731896</td>\n",
       "      <td>0.935407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quantized</td>\n",
       "      <td>39037440</td>\n",
       "      <td>39036672</td>\n",
       "      <td>0.830080</td>\n",
       "      <td>0.709022</td>\n",
       "      <td>0.714616</td>\n",
       "      <td>0.724792</td>\n",
       "      <td>0.718710</td>\n",
       "      <td>0.927124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model  params_dense  params_nonzero  eval_loss  eval_accuracy  \\\n",
       "0     Pruned     124649477        99168312   1.292657       0.580661   \n",
       "1  Distilled      82122245        82121477   0.576013       0.799401   \n",
       "2   Baseline     124649477       124648709   0.788653       0.722708   \n",
       "3  Quantized      39037440        39036672   0.830080       0.709022   \n",
       "\n",
       "   eval_precision  eval_recall   eval_f1  eval_auc  \n",
       "0        0.714066     0.527425  0.513991  0.896321  \n",
       "1        0.801039     0.821834  0.805277  0.964596  \n",
       "2        0.724727     0.748257  0.731896  0.935407  \n",
       "3        0.714616     0.724792  0.718710  0.927124  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57a646da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params_dense</th>\n",
       "      <th>params_nonzero</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pruned</td>\n",
       "      <td>124649477</td>\n",
       "      <td>99168312</td>\n",
       "      <td>1.292657</td>\n",
       "      <td>0.580661</td>\n",
       "      <td>0.714066</td>\n",
       "      <td>0.527425</td>\n",
       "      <td>0.513991</td>\n",
       "      <td>0.896321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Distilled</td>\n",
       "      <td>82122245</td>\n",
       "      <td>82121477</td>\n",
       "      <td>0.576013</td>\n",
       "      <td>0.799401</td>\n",
       "      <td>0.801039</td>\n",
       "      <td>0.821834</td>\n",
       "      <td>0.805277</td>\n",
       "      <td>0.964596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>124649477</td>\n",
       "      <td>124648709</td>\n",
       "      <td>0.788653</td>\n",
       "      <td>0.722708</td>\n",
       "      <td>0.724727</td>\n",
       "      <td>0.748257</td>\n",
       "      <td>0.731896</td>\n",
       "      <td>0.935407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quantized</td>\n",
       "      <td>39037440</td>\n",
       "      <td>39036672</td>\n",
       "      <td>0.830080</td>\n",
       "      <td>0.709022</td>\n",
       "      <td>0.714616</td>\n",
       "      <td>0.724792</td>\n",
       "      <td>0.718710</td>\n",
       "      <td>0.927124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model  params_dense  params_nonzero  eval_loss  eval_accuracy  \\\n",
       "0     Pruned     124649477        99168312   1.292657       0.580661   \n",
       "1  Distilled      82122245        82121477   0.576013       0.799401   \n",
       "2   Baseline     124649477       124648709   0.788653       0.722708   \n",
       "3  Quantized      39037440        39036672   0.830080       0.709022   \n",
       "\n",
       "   eval_precision  eval_recall   eval_f1  eval_auc  \n",
       "0        0.714066     0.527425  0.513991  0.896321  \n",
       "1        0.801039     0.821834  0.805277  0.964596  \n",
       "2        0.724727     0.748257  0.731896  0.935407  \n",
       "3        0.714616     0.724792  0.718710  0.927124  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.to_csv(f\"{output_model_path}/comparison.csv\")\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
