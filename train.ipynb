{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3129497-4160-4af9-ad70-7fc1c40da907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import optuna\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "#from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49229453-51c0-4692-b178-32346d02a29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cd4c40-f8b0-44b0-948a-dbeac84df1d0",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb92993-538b-484d-a161-eff6974a663d",
   "metadata": {},
   "source": [
    "The dataset used in this project is sourced from [Kaggle - Coronavirus tweets NLP](https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa61b86-fb34-4516-93e9-a87e9c00d124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1  advice Talk to your neighbours family to excha...            Positive  \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3  My food stock is not the only one which is emp...            Positive  \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df_train = pd.read_csv(\"Corona_NLP_train.csv\", encoding='latin1')\n",
    "df_test = pd.read_csv(\"Corona_NLP_test.csv\", encoding='latin1')\n",
    "\n",
    "# Display first few rows\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db0d07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>1</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>44953</td>\n",
       "      <td>NYC</td>\n",
       "      <td>02/03/2020</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44954</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>02/03/2020</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>44955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02/03/2020</td>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>44956</td>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>02/03/2020</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>44957</td>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>03/03/2020</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName             Location     TweetAt  \\\n",
       "0         1       44953                  NYC  02/03/2020   \n",
       "1         2       44954          Seattle, WA  02/03/2020   \n",
       "2         3       44955                  NaN  02/03/2020   \n",
       "3         4       44956          Chicagoland  02/03/2020   \n",
       "4         5       44957  Melbourne, Victoria  03/03/2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  TRENDING: New Yorkers encounter empty supermar...  Extremely Negative   \n",
       "1  When I couldn't find hand sanitizer at Fred Me...            Positive   \n",
       "2  Find out how you can protect yourself and love...  Extremely Positive   \n",
       "3  #Panic buying hits #NewYork City as anxious sh...            Negative   \n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral   \n",
       "\n",
       "   Unnamed: 6   1  Label  \n",
       "0         NaN  23      0  \n",
       "1         NaN  30      3  \n",
       "2         NaN  13      4  \n",
       "3         NaN  35      1  \n",
       "4         NaN  26      2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map sentiment to integers\n",
    "sentiment_map = {\n",
    "    'Extremely Negative': 0,\n",
    "    'Negative': 1,\n",
    "    'Neutral': 2,\n",
    "    'Positive': 3,\n",
    "    'Extremely Positive': 4\n",
    "}\n",
    "\n",
    "df_train['Label'] = df_train['Sentiment'].map(sentiment_map)\n",
    "df_test['Label'] = df_test['Sentiment'].map(sentiment_map)\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff8c827-91cf-4d35-8fb6-f4d2786853bf",
   "metadata": {},
   "source": [
    "#### Split the training data into train and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47867911-48e8-49ee-b67a-85ddb93b0ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, eval_df = train_test_split(\n",
    "    df_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df_train['Label']\n",
    ")\n",
    "\n",
    "train_df = train_df[['OriginalTweet', 'Label']].reset_index(drop=True)\n",
    "eval_df = eval_df[['OriginalTweet', 'Label']].reset_index(drop=True)\n",
    "test_df = df_test[['OriginalTweet', 'Label']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be809608-1ac1-47ed-8eec-2ea2139104b4",
   "metadata": {},
   "source": [
    "### Look at the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a40fa7-8ece-4b5d-b317-b32e4c3a54d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained('roberta-large', num_labels=5).to(device)\n",
    "model # Lets just look at the structure of the reoerta model from HF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4c5bea-d5b1-41e1-8b35-797b03f0507e",
   "metadata": {},
   "source": [
    "### Check how the data is tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e74f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count    41157.000000\n",
       " mean        58.824307\n",
       " std         23.142010\n",
       " min          5.000000\n",
       " 25%         42.000000\n",
       " 50%         58.000000\n",
       " 75%         74.000000\n",
       " max        184.000000\n",
       " Name: OriginalTweet, dtype: float64,\n",
       " {0.9: 89.0, 0.95: 98.0, 0.99: 118.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check num of tokens to choose truncation max_length\n",
    "tok = RobertaTokenizer.from_pretrained(\"roberta-large\", use_fast=True)\n",
    "lens = df_train['OriginalTweet'].astype(str).map(lambda t: len(tok.encode(t, add_special_tokens=True)))\n",
    "lens.describe(), lens.quantile([0.90, 0.95, 0.99]).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d221302",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup & Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b093444-596b-4b0f-80ac-99c1441e8349",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.texts = dataframe['OriginalTweet'].tolist()\n",
    "        self.labels = dataframe['Label'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c55f7255-ae13-4e50-9344-7d0ef8628cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stop_check(patience, best_val_accuracy, best_val_accuracy_epoch, current_val_accuracy, current_val_accuracy_epoch):\n",
    "    early_stop_flag = False\n",
    "    if current_val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = current_val_accuracy\n",
    "        best_val_accuracy_epoch = current_val_accuracy_epoch\n",
    "    else:\n",
    "        if current_val_accuracy_epoch - best_val_accuracy_epoch > patience:\n",
    "            early_stop_flag = True\n",
    "    return best_val_accuracy, best_val_accuracy_epoch, early_stop_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a3284d8-be32-4b33-b6a8-0d214c31ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_hyperparams(model, train_loader, val_loader, optimizer, criterion, epochs, patience, trial, scheduler):\n",
    "    best_val_accuracy = 0.0\n",
    "    best_val_accuracy_epoch = 0\n",
    "    early_stop_flag = False\n",
    "    best_model_state = None\n",
    "\n",
    "    # Enable automatic mixed precision on CUDA for stability/speed\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=(device.type == \"cuda\"))\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train() # Enable training mode\n",
    "        train_loss = 0.0\n",
    "        total_train_samples = 0\n",
    "        correct_train_predictions = 0\n",
    "\n",
    "        for batch in train_loader: #Iterates over the train_loader, which is a DataLoader object containing batches of training data.\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True) # Reset gradients\n",
    "\n",
    "            # Forward pass (with AMP); save the logits (the raw output of the model) and calculate loss\n",
    "            with torch.amp.autocast(\"cuda\", enabled=(device.type == \"cuda\")):\n",
    "                outputs = model(input_ids, attention_mask=attention_mask) # Forward pass\n",
    "                logits = outputs.logits\n",
    "                loss = criterion(logits, labels) # Calculate loss\n",
    "\n",
    "            # Backward pass (with AMP) + gradient clipping, then update weights using the optimizer\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # NEW: prevent exploding gradients\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # NEW: step the LR scheduler once per optimizer step\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            # Accumulate training loss and predictions\n",
    "            train_loss += loss.item() * input_ids.size(0)\n",
    "            total_train_samples += input_ids.size(0)\n",
    "            correct_train_predictions += (logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "        train_loss /= total_train_samples\n",
    "        train_accuracy = correct_train_predictions / total_train_samples\n",
    "\n",
    "        ###  Validation loop  ###\n",
    "        model.eval() # Enable evaluation mode\n",
    "        val_loss = 0.0\n",
    "        total_val_samples = 0\n",
    "        correct_val_predictions = 0\n",
    "\n",
    "        all_val_labels = []\n",
    "        all_val_preds = []\n",
    "\n",
    "        with torch.no_grad(): # Disable gradient computation\n",
    "            for batch in val_loader: # iterate on the val_loader's batches \n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "                val_loss += loss.item() * input_ids.size(0)\n",
    "                total_val_samples += input_ids.size(0)\n",
    "                correct_val_predictions += (logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "                all_val_labels.extend(labels.cpu().numpy())\n",
    "                all_val_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        # calculate metrics \n",
    "        val_loss /= total_val_samples\n",
    "        val_accuracy = correct_val_predictions / total_val_samples\n",
    "        val_precision = precision_score(all_val_labels, all_val_preds, average='weighted')\n",
    "        val_recall = recall_score(all_val_labels, all_val_preds, average='weighted')\n",
    "        val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted')\n",
    "\n",
    "        # Check for early stopping (UNCHANGED: still based on accuracy)\n",
    "        best_val_accuracy, best_val_accuracy_epoch, early_stop_flag = early_stop_check(\n",
    "            patience, best_val_accuracy, best_val_accuracy_epoch, val_accuracy, epoch\n",
    "        )\n",
    "\n",
    "        # Save the best model under the best_model_state parameter (UNCHANGED: still by accuracy)\n",
    "        if val_accuracy == best_val_accuracy:\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        # Log metrics to Weights & Biases - THIS IS WHERE WE TRACK THE RESULTS AND THE PROCESS\n",
    "        wandb.log({ #log == logging of the training process (e.g. results) - will be done each epoch\n",
    "            \"Epoch\": epoch,\n",
    "            \"Train Loss\": train_loss,\n",
    "            \"Train Accuracy\": train_accuracy,\n",
    "            \"Validation Loss\": val_loss,\n",
    "            \"Validation Accuracy\": val_accuracy,\n",
    "            \"Validation Precision\": val_precision,\n",
    "            \"Validation Recall\": val_recall,\n",
    "            \"Validation F1\": val_f1\n",
    "        })\n",
    "\n",
    "        if early_stop_flag:  # Checks whether the early stopping condition has been met, as indicated by the early_stop_flag\n",
    "            break # Exits the training loop immediately if the early stopping condition is satisfied\n",
    "\n",
    "    if best_model_state is not None: # Save the best model as a .pt file\n",
    "        torch.save(best_model_state, f\"best_model_trial_{trial.number}.pt\")\n",
    "\n",
    "    return best_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3214c37-4ebf-48bc-8c94-8dcc30199bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\", use_fast=True)\n",
    "\n",
    "# Objective Function for Optuna\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 3e-5)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 5e-3, 2e-2)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])  # use grad accumulation if VRAM is tight\n",
    "    patience   = trial.suggest_int(\"patience\", 4, 8)\n",
    "    num_layers = trial.suggest_categorical(\"num_layers\", [4, 6, 8, 12])\n",
    "\n",
    "    train_dataset = TweetDataset(train_df, tokenizer) # Create the TweetDataset object\n",
    "    val_dataset = TweetDataset(eval_df, tokenizer)    # Create the TweetDataset object\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=6, pin_memory=True, persistent_workers=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,   # NEW: no shuffle for validation\n",
    "                            num_workers=6, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "    model = RobertaForSequenceClassification.from_pretrained('roberta-large', num_labels=5).to(device) # initialize RoBerta large from HF, num_labels=5 -> 5 classes.\n",
    "\n",
    "    for param in model.roberta.parameters():    # Freeze layers\n",
    "        param.requires_grad = False\n",
    "    for param in model.roberta.encoder.layer[-num_layers:].parameters():     # unfreeze the last \"num_layers\" of the encoder\n",
    "        param.requires_grad = True\n",
    "    for param in model.classifier.parameters():    #unfreeze the classifier\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    # NEW: AdamW optimizer (transformer-friendly) instead of plain Adam\n",
    "    from torch.optim import AdamW\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # NEW: class-weighted CrossEntropy to handle label imbalance\n",
    "    counts = train_df['Label'].value_counts().sort_index().values\n",
    "    weights = torch.tensor((counts.sum() / (counts + 1e-9)), dtype=torch.float32, device=device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "    # NEW: LR scheduler with warmup (linear decay)\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    epochs = 20  # keep your epoch budget here so we can compute total steps\n",
    "    num_training_steps = epochs * len(train_loader)\n",
    "    num_warmup_steps = int(0.06 * num_training_steps)  # ~6% warmup\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "\n",
    "    # Initialize Weights & Biases - the values in the config are the properties of each trial.\n",
    "    wandb.init(project=\"roberta-covid-sentiment-maxlen128\",\n",
    "               config={ \n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"patience\": patience,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"architecture\": \"RoBERTa\",\n",
    "        \"dataset\": \"COVID-19 NLP\"}, \n",
    "        name=f\"trial_{trial.number}\") # The name that will be saved in the W&B platform\n",
    "\n",
    "    # Train the model and get the best validation accuracy\n",
    "    best_val_accuracy = train_model_with_hyperparams(\n",
    "        model, train_loader, val_loader, optimizer, criterion,\n",
    "        epochs=epochs, patience=patience, trial=trial, scheduler=scheduler\n",
    "    )\n",
    "\n",
    "    wandb.finish() # Finish the Weights & Biases run\n",
    "    \n",
    "    return best_val_accuracy # Return best validation acc as the objective to maximize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d569ea-b1a9-4e2d-909b-8b6de30b1d4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 20:45:57,049] A new study created in memory with name: no-name-592a5201-be08-42bd-b454-70dd2e16a828\n",
      "/tmp/ipykernel_1860183/2678458747.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 3e-5)\n",
      "/tmp/ipykernel_1860183/2678458747.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 5e-3, 2e-2)\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myoyulia\u001b[0m (\u001b[33myoyulia-tel-aviv-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/idok/random/julia/MLproj/JIC/wandb/run-20250812_204558-qyjhtjv0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128/runs/qyjhtjv0' target=\"_blank\">trial_0</a></strong> to <a href='https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128' target=\"_blank\">https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128/runs/qyjhtjv0' target=\"_blank\">https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128/runs/qyjhtjv0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▄▅▅▆▆▆▇▇▇▇▇▇███████</td></tr><tr><td>Train Loss</td><td>█▆▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▄▅▆▆▇▇▇▇▇▇█▇█▇█████</td></tr><tr><td>Validation F1</td><td>▁▄▅▇▆▇▇▇█▇██▇███████</td></tr><tr><td>Validation Loss</td><td>█▄▃▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂</td></tr><tr><td>Validation Precision</td><td>▁▄▅▆▆▇▇▇▇▇██████████</td></tr><tr><td>Validation Recall</td><td>▁▄▅▆▆▇▇▇▇▇▇█▇█▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>20</td></tr><tr><td>Train Accuracy</td><td>0.83204</td></tr><tr><td>Train Loss</td><td>0.40595</td></tr><tr><td>Validation Accuracy</td><td>0.71016</td></tr><tr><td>Validation F1</td><td>0.70813</td></tr><tr><td>Validation Loss</td><td>0.79373</td></tr><tr><td>Validation Precision</td><td>0.71337</td></tr><tr><td>Validation Recall</td><td>0.71016</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_0</strong> at: <a href='https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128/runs/qyjhtjv0' target=\"_blank\">https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128/runs/qyjhtjv0</a><br> View project at: <a href='https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128' target=\"_blank\">https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250812_204558-qyjhtjv0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 22:10:39,622] Trial 0 finished with value: 0.7175655976676385 and parameters: {'learning_rate': 1.8084585310083114e-05, 'weight_decay': 0.009508208736324634, 'batch_size': 32, 'patience': 5, 'num_layers': 6}. Best is trial 0 with value: 0.7175655976676385.\n",
      "/tmp/ipykernel_1860183/2678458747.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 3e-5)\n",
      "/tmp/ipykernel_1860183/2678458747.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 5e-3, 2e-2)\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/idok/random/julia/MLproj/JIC/wandb/run-20250812_221040-xfkh2t9u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128/runs/xfkh2t9u' target=\"_blank\">trial_1</a></strong> to <a href='https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128' target=\"_blank\">https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128/runs/xfkh2t9u' target=\"_blank\">https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128/runs/xfkh2t9u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▄▅▅▆▆▇▇▇▇▇█████████</td></tr><tr><td>Train Loss</td><td>█▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▄▅▆▇▇▇▇▇▇▇█▇██▇████</td></tr><tr><td>Validation F1</td><td>▁▄▅▆▇▇▇▇▇▇▇█▇██▇████</td></tr><tr><td>Validation Loss</td><td>▃▂▂▁▁▁▁▁▁▂▂▂▃▄▅▇▇███</td></tr><tr><td>Validation Precision</td><td>▁▄▅▆▇▇▇▇▇█▇█▇███████</td></tr><tr><td>Validation Recall</td><td>▁▄▅▆▇▇▇▇▇▇▇█▇██▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>20</td></tr><tr><td>Train Accuracy</td><td>0.98147</td></tr><tr><td>Train Loss</td><td>0.14128</td></tr><tr><td>Validation Accuracy</td><td>0.8122</td></tr><tr><td>Validation F1</td><td>0.8118</td></tr><tr><td>Validation Loss</td><td>2.17918</td></tr><tr><td>Validation Precision</td><td>0.81256</td></tr><tr><td>Validation Recall</td><td>0.8122</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_1</strong> at: <a href='https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128/runs/xfkh2t9u' target=\"_blank\">https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128/runs/xfkh2t9u</a><br> View project at: <a href='https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128' target=\"_blank\">https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250812_221040-xfkh2t9u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 00:19:05,109] Trial 1 finished with value: 0.8134110787172012 and parameters: {'learning_rate': 1.6189611749289203e-05, 'weight_decay': 0.008651522228555737, 'batch_size': 16, 'patience': 6, 'num_layers': 12}. Best is trial 1 with value: 0.8134110787172012.\n",
      "/tmp/ipykernel_1860183/2678458747.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 3e-5)\n",
      "/tmp/ipykernel_1860183/2678458747.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 5e-3, 2e-2)\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/idok/random/julia/MLproj/JIC/wandb/run-20250813_001906-16ajmebf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128/runs/16ajmebf' target=\"_blank\">trial_2</a></strong> to <a href='https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128' target=\"_blank\">https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128/runs/16ajmebf' target=\"_blank\">https://wandb.ai/yoyulia-tel-aviv-university/roberta-covid-sentiment-maxlen128/runs/16ajmebf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Optuna Study\n",
    "study = optuna.create_study(direction=\"maximize\")  # Specifies that the goal of the optimization is to maximize the objective function\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906a6c66-88e3-4c55-b30e-6b2d8a67dda7",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f5006a-d8b8-4d77-9e8e-908477135903",
   "metadata": {},
   "source": [
    "# Testing - note didn't run yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b7dba8-0aaf-406b-acdd-7218e54b06c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(model_path, test_loader):\n",
    "    # Load the model\n",
    "    model = RobertaForSequenceClassification.from_pretrained('roberta-large', num_labels=5)\n",
    "    model.load_state_dict(torch.load(model_path)) # loading the trained model\n",
    "    model = model.to(device)\n",
    "    model.eval() # eval mode\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    #same idea... just testing and getting resuts...\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = logits.argmax(dim=1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    return {\"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1 Score\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73efe752-1581-42b5-a607-e3de3e30934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data set\n",
    "test_dataset = DataLoader(test_df, RobertaTokenizer.from_pretrained('roberta-large'))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed8fef3-014e-429e-a274-b897ca3db0e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test multiple models\n",
    "model_paths = [\"best_model_trial_0.pt\"]  # Replace with actual model paths\n",
    "for model_path in model_paths:\n",
    "    metrics = evaluate_model(model_path, test_loader)\n",
    "    print(f\"Metrics for {model_path}:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
